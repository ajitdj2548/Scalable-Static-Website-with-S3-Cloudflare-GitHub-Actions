

Now the only left to do is copy the files from your statically generated website&mdash;this tutorial's example will be the files in the __public__ folder generated by Hugo.


## COPY STATIC WEBSITE FILES TO S3

Continuing with our Hugo example, go ahead and generate the actual website using the `hugo -v` command. This will create a new folder called __public__:

```
hugo -v
```


Drag over the files and press the __Upload__ button:



### CLI METHOD

If you use this method you can upload your statically generated website via the AWS CLI. This might be preferable since you will be at the command line anyway however, you have to generate AWS Access Keys from the console.


At the command prompt, run the command `aws configure` to add a profile for this limited IAM user. Use the data from the __Create access key__ for the __Access key ID__ and __Secret access key__. I usually don't store the Secret access key anywhere else (it's stored in plaintext on your local machine) and I don't worry about losing it as I can just disable the lost one and create a new one:



Run the local AWS CLI configuration command&mdash;make sure to use a profile especially if you already have a default profile configured. Note that the example below has masked the actual ID and secret:
```


Now that you have Access keys configured try listing your static website bucket:

```
aws s3 ls tutorialstuff.xyz --profile tutorialstuff-xyz
```
the work because the IAM user that you enabled access keys for is part of the group that has a policy action of `s3:GetObject` set to `allow` for this specific S3 bucket.

You can delete the public bucket policy because we will set objects to public when we upload them via the AWS CLI:
